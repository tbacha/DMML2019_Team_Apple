{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Insight from Kickstarter Data: Machine Learning\n",
    "\n",
    "_A project by Team Apple (Data Mining & Machine Learning, HEC Lausanne, Fall 2019)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is dedicated to training machine learning models on the cleaned Kickstarter dataset, in order to find out which model (if any at all), and which features, can accurately predict the success or failure of a project.\n",
    "\n",
    "**Contents**\n",
    "\n",
    "1. [Imports](#imports)\n",
    "2. [Machine learning models](#ml)\n",
    "    1. [Logistic regression](#logr)\n",
    "    2. [Decision tree and random forest](#dtrf)\n",
    "    3. [k-nearest neighbors](#knn)\n",
    "    4. [Neural network](#nn)\n",
    "    5. [Linear regression](#linr)\n",
    "3. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & installations<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\programdata\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.2.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (5.1.1)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: tensorflow==1.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.25.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (0.33.4)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.15.0,>=1.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (3.11.1)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==1.14.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (41.0.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (0.15.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from cleaning import df\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Machine learning models<a name=\"ml\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The classes to be predicted are either **1** (= successful) or **0** (= failed).\n",
    " \n",
    " As a reminder, the base rates are 36.3 and 63.7% respectively.\n",
    " \n",
    " We start our research using the first model for classification that we've seen in the course, logistic regression. We will experiment with other classification models to find the most accurate ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric features are normalized between 0 and 1 in order to have a better basis for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code can be used should normalization be necessary\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[\"usd_goal_real\"] = pd.DataFrame(scaler.fit_transform(df[\"usd_goal_real\"].to_numpy().reshape(-1, 1)))\n",
    "df[\"elapsed_time\"] = pd.DataFrame(scaler.fit_transform(pd.to_numeric(df[\"elapsed_time\"]).to_numpy().reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are setting the random state to an arbitrarily chosen number to enable comparison between models and their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mtrand.RandomState at 0x243ce0587e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.RandomState(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Logistic regression<a name=\"logr\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to see if our dataset is coherent, we create and test a model to see how accurate the goal and the amount of money pledged are to predict the success or failure. If the dataset is coherent, this number should be very close to 100% and indeed, we reach near-perfection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving beyond the obvious, we start including other features such as the category and the country. We remove the amount pledged, since this is not a variable that the project creater has direct control over. Since they are categorical data, we need to use a one-hot encoder so that the regression model can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base rate = 0.6365718669220111\n",
    "\n",
    "first use LR (first method seen in class) to explore how different features affect accuracy.\n",
    "\n",
    "After having played with the different features and training/testing multiple times, it turns out the specific category is the one that brings the highest marginal increase in accuracy. (but since lot of values, slow runtime --> possible to pick only maincat, but not as good accuracy) We also find that OH performs much better than LE.\n",
    "\n",
    "It turns out simply picking the cat-main cat pair is what's more relevant to increase accuracy.\n",
    "\n",
    "one must normalize time to increase accuracy\n",
    "\n",
    "not only do goal and time not bring more accuracy (more or less = to base rate, they actually decrease it when used with more relevant features.\n",
    "\n",
    "decide do keep only category and main category, which seem to work well together. Adding country gives similar scores, so no need to include it (to speed up runtime).\n",
    "best accuracy for LR : 0.6698584625357895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame()\n",
    "X = df[[\"usd_goal_real\", \"elapsed_time\"]]\n",
    "y = df[\"state\"]\n",
    "\n",
    "one_hot = OneHotEncoder()\n",
    "cat_to_onehot = pd.DataFrame(one_hot.fit_transform(df[[\"category\", \"main_category\", \"country\"]]).toarray())\n",
    "X = pd.concat((X, cat_to_onehot), axis=1)\n",
    "\n",
    "\"\"\"\n",
    "le = LabelEncoder()\n",
    "for col in [\"category\", \"main_category\"]:\n",
    "    encoded = pd.DataFrame(le.fit_transform(df[col]), columns=[col])\n",
    "    X = pd.concat((X, encoded), axis=1)\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6661984765814921"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = LogisticRegressionCV(solver=\"lbfgs\", cv=5, max_iter=1000)\n",
    "LR.fit(X_train, y_train)\n",
    "LR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, LR.predict(X_test), output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Decision tree and random forest<a name=\"dtrf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also tried numerical features with decision trees and random forests. max 65.9% for both (tree more consistent across values of depth, but max is the same).\n",
    "\n",
    "gini or entropy perform similarly\n",
    "\n",
    "with the same features that we kept for LR (cat and maincat), reach a score of 66.57 (same as LR), but that score is reached faster!\n",
    "\n",
    "LR:0.6657392901518017\n",
    "DT:0.6657122791853493\n",
    "RF:0.6645778185943493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{18: 0.6657122791853493}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {}\n",
    "\n",
    "for d in range(18,19):\n",
    "    DT = DecisionTreeClassifier(criterion=\"entropy\", max_depth=d)\n",
    "    DT.fit(X_train, y_train)\n",
    "    scores[d] = DT.score(X_test, y_test)\n",
    "\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{22: 0.6614040300361947,\n",
       " 23: 0.6614040300361947,\n",
       " 24: 0.6645778185943493,\n",
       " 25: 0.6621738425800875}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {}\n",
    "for d in range(22,26):\n",
    "    RF = RandomForestClassifier(criterion=\"entropy\", n_estimators=15, max_depth=d, random_state=RSEED)\n",
    "    RF.fit(X_train, y_train)\n",
    "    scores[d] = RF.score(X_test, y_test)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. KNN<a name=\"knn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "kMax=0\n",
    "for k in range(1, 100, 1):\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))\n",
    "    if clf.score(X_test, y_test) >= max(scores):\n",
    "      kMax = k\n",
    "plt.plot(range(1, 100, 1), scores)\n",
    "plt.ylabel('accuracy', fontsize=15)\n",
    "plt.xlabel('$k (knn2)$', fontsize=15)\n",
    "print(\"kMax: \",kMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Neural network<a name=\"nn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest: 67.12% with cat and maincat\n",
    "adding country: 66.67\n",
    "\n",
    "with numerical values it stays low.\n",
    "\n",
    "all features: 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               89600     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 90,626\n",
      "Trainable params: 90,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, 2)\n",
    "Y_test = np_utils.to_categorical(y_test, 2)\n",
    "\n",
    "NN = Sequential()\n",
    "NN.add(Dense(512, input_shape=(X.shape[1],)))\n",
    "NN.add(Activation(\"relu\"))\n",
    "NN.add(Dropout(0.2))\n",
    "NN.add(Dense(2))\n",
    "NN.add(Activation(\"softmax\"))\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "NN.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 236940 samples, validate on 59235 samples\n",
      "Epoch 1/30\n",
      "236940/236940 [==============================] - 17s 71us/step - loss: 0.6527 - accuracy: 0.6346 - val_loss: 0.6449 - val_accuracy: 0.6359\n",
      "Epoch 2/30\n",
      "236940/236940 [==============================] - 16s 69us/step - loss: 0.6416 - accuracy: 0.6365 - val_loss: 0.6380 - val_accuracy: 0.6359\n",
      "Epoch 3/30\n",
      "236940/236940 [==============================] - 16s 69us/step - loss: 0.6358 - accuracy: 0.6371 - val_loss: 0.6331 - val_accuracy: 0.6359\n",
      "Epoch 4/30\n",
      "236940/236940 [==============================] - 16s 69us/step - loss: 0.6317 - accuracy: 0.6397 - val_loss: 0.6295 - val_accuracy: 0.6402\n",
      "Epoch 5/30\n",
      "236940/236940 [==============================] - 16s 70us/step - loss: 0.6286 - accuracy: 0.6441 - val_loss: 0.6267 - val_accuracy: 0.6513\n",
      "Epoch 6/30\n",
      "236940/236940 [==============================] - 16s 69us/step - loss: 0.6263 - accuracy: 0.6484 - val_loss: 0.6245 - val_accuracy: 0.6507\n",
      "Epoch 7/30\n",
      "236940/236940 [==============================] - 17s 71us/step - loss: 0.6244 - accuracy: 0.6524 - val_loss: 0.6227 - val_accuracy: 0.6546\n",
      "Epoch 8/30\n",
      "236940/236940 [==============================] - 16s 67us/step - loss: 0.6229 - accuracy: 0.6552 - val_loss: 0.6213 - val_accuracy: 0.6591\n",
      "Epoch 9/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6217 - accuracy: 0.6570 - val_loss: 0.6202 - val_accuracy: 0.6597\n",
      "Epoch 10/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6206 - accuracy: 0.6584 - val_loss: 0.6191 - val_accuracy: 0.6596\n",
      "Epoch 11/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6197 - accuracy: 0.6603 - val_loss: 0.6183 - val_accuracy: 0.6634\n",
      "Epoch 12/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6190 - accuracy: 0.6611 - val_loss: 0.6175 - val_accuracy: 0.6634\n",
      "Epoch 13/30\n",
      "236940/236940 [==============================] - 17s 70us/step - loss: 0.6185 - accuracy: 0.6623 - val_loss: 0.6169 - val_accuracy: 0.6645\n",
      "Epoch 14/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6177 - accuracy: 0.6633 - val_loss: 0.6163 - val_accuracy: 0.6647\n",
      "Epoch 15/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6173 - accuracy: 0.6629 - val_loss: 0.6158 - val_accuracy: 0.6659\n",
      "Epoch 16/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6169 - accuracy: 0.6643 - val_loss: 0.6153 - val_accuracy: 0.6660\n",
      "Epoch 17/30\n",
      "236940/236940 [==============================] - 16s 67us/step - loss: 0.6164 - accuracy: 0.6648 - val_loss: 0.6149 - val_accuracy: 0.6664\n",
      "Epoch 18/30\n",
      "236940/236940 [==============================] - 16s 70us/step - loss: 0.6160 - accuracy: 0.6645 - val_loss: 0.6145 - val_accuracy: 0.6675\n",
      "Epoch 19/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6156 - accuracy: 0.6653 - val_loss: 0.6142 - val_accuracy: 0.6664\n",
      "Epoch 20/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6154 - accuracy: 0.6649 - val_loss: 0.6139 - val_accuracy: 0.6675\n",
      "Epoch 21/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6149 - accuracy: 0.6657 - val_loss: 0.6135 - val_accuracy: 0.6679\n",
      "Epoch 22/30\n",
      "236940/236940 [==============================] - 17s 70us/step - loss: 0.6149 - accuracy: 0.6664 - val_loss: 0.6133 - val_accuracy: 0.6696\n",
      "Epoch 23/30\n",
      "236940/236940 [==============================] - 16s 70us/step - loss: 0.6146 - accuracy: 0.6659 - val_loss: 0.6131 - val_accuracy: 0.6688\n",
      "Epoch 24/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6145 - accuracy: 0.6661 - val_loss: 0.6128 - val_accuracy: 0.6695\n",
      "Epoch 25/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6142 - accuracy: 0.6663 - val_loss: 0.6126 - val_accuracy: 0.6695\n",
      "Epoch 26/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6139 - accuracy: 0.6664 - val_loss: 0.6124 - val_accuracy: 0.6699\n",
      "Epoch 27/30\n",
      "236940/236940 [==============================] - 16s 67us/step - loss: 0.6137 - accuracy: 0.6668 - val_loss: 0.6122 - val_accuracy: 0.6699\n",
      "Epoch 28/30\n",
      "236940/236940 [==============================] - 16s 67us/step - loss: 0.6134 - accuracy: 0.6671 - val_loss: 0.6120 - val_accuracy: 0.6700\n",
      "Epoch 29/30\n",
      "236940/236940 [==============================] - 16s 68us/step - loss: 0.6132 - accuracy: 0.6669 - val_loss: 0.6119 - val_accuracy: 0.6700\n",
      "Epoch 30/30\n",
      "236940/236940 [==============================] - 16s 66us/step - loss: 0.6132 - accuracy: 0.6668 - val_loss: 0.6117 - val_accuracy: 0.6700\n"
     ]
    }
   ],
   "source": [
    "model_hist = NN.fit(X_train, Y_train, batch_size=64, epochs=30, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Linear regression<a name=\"linr\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion<a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
